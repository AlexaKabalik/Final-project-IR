{"cells":[{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"hWgiQS0zkWJ5"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"code","execution_count":null,"id":"5ac36d3a","metadata":{"id":"c0ccf76b","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"01ec9fd3"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":null,"id":"bf199e6a","metadata":{"id":"32b3ec57","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":null,"id":"d8f56ecd","metadata":{"id":"5609143b","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","import json\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"id":"38a897f2","metadata":{"id":"b10cc999","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":null,"id":"47900073","metadata":{"id":"d3f86f11","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","import pyspark.sql.functions as pyfunc\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":null,"id":"72bed56b","metadata":{"id":"5be6dc2a","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[],"source":["spark"]},{"cell_type":"code","execution_count":null,"id":"980e62a5","metadata":{"id":"7adc1bf5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'inverted_index_creation' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","#     print(b.name)\n","    if ((b.name != 'graphframes.sh') and (\"index/\" not in b.name ) and (\"postings_gcp/\" not in b.name) and (\"page_rank/\" not in b.name)):\n","        paths.append(full_path+b.name)\n","paths"]},{"cell_type":"markdown","id":"481f2044","metadata":{"id":"02f81c72"},"source":["## Extracting title, anchor, text"]},{"cell_type":"code","execution_count":null,"id":"e4c523e7","metadata":{"id":"b1af29c9","scrolled":false},"outputs":[],"source":["parquetFile = spark.read.parquet(*paths)\n","\n","doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n","doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n","pages_links = parquetFile.select(\"anchor_text\",\"id\").rdd"]},{"cell_type":"markdown","id":"0d7e2971","metadata":{"id":"f6375562"},"source":["We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"]},{"cell_type":"code","execution_count":null,"id":"82881fbf","metadata":{"id":"d89a7a9a"},"outputs":[],"source":["# Count number of wiki pages\n","parquetFile.count()"]},{"cell_type":"markdown","id":"701811af","metadata":{"id":"gaaIoFViXyTg"},"source":["Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."]},{"cell_type":"code","execution_count":null,"id":"121fe102","metadata":{"id":"04371c88","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":null,"id":"57c101a8","metadata":{"id":"2d3285d8","scrolled":true},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":null,"id":"c259c402","metadata":{"id":"2477a5b9"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"code","execution_count":null,"id":"1a9d1e19","metadata":{},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)"]},{"cell_type":"code","execution_count":null,"id":"32da41ae","metadata":{},"outputs":[],"source":["# with open('queries_train_partial.json') as f:\n","#     data = json.load(f)\n","\n","# filtered_sorted_query_tokens=[token.group() for token in RE_WORD.finditer(''.join(data.keys()).lower())]\n","# query_trains_without_stopwords=set([word for word in filtered_sorted_query_tokens if word not in all_stopwords])"]},{"cell_type":"code","execution_count":null,"id":"f3ad8fea","metadata":{"id":"a4b6ee29","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","def word_count(text, id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","#     filtered_tokens = [word for word in tokens if (word not in all_stopwords) and (word in query_trains_without_stopwords)]\n","    filtered_tokens = [word for word in tokens if (word not in all_stopwords)]\n","    word_tf = Counter(filtered_tokens)\n","    return [(word,(id,tf)) for word,tf in word_tf.items()]\n","\n","def reduce_word_counts(unsorted_pl):\n","    return(sorted(unsorted_pl, key = lambda x: x[0], reverse=False))\n","\n","def anchor_word_count(text,id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    filtered_tokens = [word for word in tokens if (word not in all_stopwords)]\n","    return [(id, filtered_tokens)]\n","\n","def reduce_anchor_word_count(anchor_text):\n","    return anchor_text\n","\n","def doc_2_len_doc(text, doc_id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    filtered_tokens = [word for word in tokens if (word not in all_stopwords)]\n","    return [(doc_id, len(filtered_tokens))]\n","\n","def reduce_doc_2_len_doc(doc_len):\n","    return doc_len\n","\n","def doc_to_title(title, doc_id):\n","    return [(doc_id, title)]\n","\n","def reduce_doc_to_title(doc_len):\n","    return doc_len\n","\n","def calculate_df(postings):\n","    return postings.mapValues(lambda token:len(token))\n","\n","def partition_postings_and_write(postings,bucket_name):\n","    inverted_index = InvertedIndex()\n","    return postings.map(lambda posting:(token2bucket_id(posting[0]),(posting[0],posting[1]))).groupByKey().map(lambda t: (t[0],list(t[1]))).map(lambda np:inverted_index.write_a_posting_list(np,bucket_name))"]},{"cell_type":"markdown","id":"a54ceb4c","metadata":{},"source":["## Title Inverted Index"]},{"cell_type":"code","execution_count":null,"id":"d1c8e425","metadata":{},"outputs":[],"source":["# create inverted index for title\n","title_index_bucket_folder = full_path+'postings_gcp'\n","title_res = doc_title_pairs.flatMap(lambda x: word_count(x[0],x[1]))\n","title_postings = title_res.groupByKey().mapValues(reduce_word_counts)\n","w2df_title=calculate_df(title_postings)\n","\n","w2df_dict_title = w2df_title.collectAsMap()\n","# stats = storage.Blob(bucket={full_path}, name=title_postings).exists(client)\n","# print(stats)\n","print(title_index_bucket_folder)\n","# try:\n","#     ! mkdir {title_index_drive_folder}\n","# except:\n","#     pass\n","title_posting_locs_list = partition_postings_and_write(title_postings,bucket_name).collect()\n","title_posting_locs_list"]},{"cell_type":"code","execution_count":null,"id":"ce53a392","metadata":{},"outputs":[],"source":["super_posting_locs_title = defaultdict(list)\n","for posting_loc in title_posting_locs_list:\n","    for k, v in posting_loc.items():\n","        super_posting_locs_title[k].extend(v)\n","super_posting_locs_title"]},{"cell_type":"code","execution_count":null,"id":"e4562942","metadata":{},"outputs":[],"source":["! gsutil mv gs://inverted_index_creation/postings_gcp/* gs://inverted_index_creation/title_postings_gcp"]},{"cell_type":"code","execution_count":null,"id":"f7266542","metadata":{},"outputs":[],"source":["# Create inverted index instance\n","inverted_title = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted_title.posting_locs = super_posting_locs_title\n","# Add the token - df dictionary to the inverted index\n","inverted_title.df = w2df_dict_title\n","# inverted_title.term_total = title_postings.map(lambda x: calculate_total_term(x[0], x[1]))\n","# inverted_title._N = parquetFile.count()\n","# write the global stats out\n","inverted_title.write_index('.', 'title_inverted_index')\n","index_src = \"title_inverted_index.pkl\"\n","index_dst = f'gs://{bucket_name}/title_postings_gcp/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"f9abd8fe","metadata":{},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex, read_posting_list\n","inverted_text = InvertedIndex.read_index(base_dir= '/home/dataproc/',name='title_inverted_index')"]},{"cell_type":"code","execution_count":null,"id":"297cf1ac","metadata":{},"outputs":[],"source":["inverted_text.posting_locs"]},{"cell_type":"markdown","id":"179c4b97","metadata":{},"source":["## Text Inverted Index"]},{"cell_type":"code","execution_count":null,"id":"be9a8d03","metadata":{},"outputs":[],"source":["word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","# partition posting lists and write out\n","posting_locs_list = partition_postings_and_write(postings_filtered,bucket_name).collect()\n","print(posting_locs_list)"]},{"cell_type":"code","execution_count":null,"id":"ee3f1cfe","metadata":{},"outputs":[],"source":["# merge the posting locations into a single dict\n","super_posting_locs = defaultdict(list)\n","for posting_loc in posting_locs_list:\n","    for k, v in posting_loc.items():\n","        super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":null,"id":"78f2b651","metadata":{},"outputs":[],"source":["! gsutil mv gs://inverted_index_creation/postings_gcp/* gs://inverted_index_creation/text_postings_gcp"]},{"cell_type":"code","execution_count":null,"id":"ba684755","metadata":{},"outputs":[],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","# write the global stats out\n","inverted.write_index('.', 'text_inverted_index')\n","index_src = \"text_inverted_index.pkl\"\n","index_dst = f'gs://{bucket_name}/text_postings_gcp/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"markdown","id":"5d0c756e","metadata":{},"source":["## Anchor Inverted Index"]},{"cell_type":"code","execution_count":null,"id":"b57f1e89","metadata":{},"outputs":[],"source":["def combine_dict_items(lst):\n","    dic={}\n","    for docs,value in lst:\n","        if docs in dic.keys():\n","            dic[docs]+=value\n","        else:\n","            dic[docs]=value\n","    return list(zip(dic.keys(),dic.values()))"]},{"cell_type":"code","execution_count":null,"id":"79354f1e","metadata":{},"outputs":[],"source":["def calc_total_word_tf(word,posting_list):\n","    total=0\n","    for _,tf in posting_list:\n","        total+=tf\n","    return word,total"]},{"cell_type":"code","execution_count":null,"id":"fb9f5030","metadata":{},"outputs":[],"source":["# anchor_rdd = pages_links.flatMap(lambda x: x[0]).flatMap(lambda x: anchor_word_count(x[1],x[0]))\n","# anchor_df = anchor_rdd.groupByKey().mapValues(reduce_anchor_word_count)\n","anchor_rdd = pages_links.flatMap(lambda y: map(lambda x: word_count(x[1],(y[1],x[0])),y[0]))\n","anchor_word_counts = anchor_rdd.flatMap(lambda x:x)\n","anchor_words_not_unif = anchor_word_counts.mapValues(lambda y:(y[0][1],y[1]))"]},{"cell_type":"code","execution_count":null,"id":"02c380d0","metadata":{},"outputs":[],"source":["anchor_postings = anchor_words_not_unif.groupByKey().mapValues(lambda x:combine_dict_items(sorted(list(x),key=lambda x:x[0])))\n","# anchor_postings_filtered = anchor_postings.filter(lambda x: len(x[1])>50)\n","# anchor_w2df = calculate_df(anchor_postings_filtered)\n","anchor_w2df = calculate_df(anchor_postings)\n","anchor_w2df_dict = anchor_w2df.collectAsMap()\n"]},{"cell_type":"code","execution_count":null,"id":"d4228722","metadata":{},"outputs":[],"source":["anchor_posting_locs_list = partition_postings_and_write(anchor_postings_filtered, bucket_name).collect()"]},{"cell_type":"code","execution_count":null,"id":"22998fb3","metadata":{},"outputs":[],"source":["anchor_super_posting_locs = defaultdict(list)\n","for posting_loc in anchor_posting_locs_list:\n","    for k, v in posting_loc.items():\n","        anchor_super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":null,"id":"77be3db5","metadata":{},"outputs":[],"source":["! gsutil mv gs://inverted_index_creation/postings_gcp/* gs://inverted_index_creation/anchor_postings_gcp"]},{"cell_type":"code","execution_count":null,"id":"24d90f10","metadata":{},"outputs":[],"source":["inverted = InvertedIndex()\n","inverted.posting_locs = anchor_super_posting_locs\n","inverted.df = anchor_w2df_dict\n","totals = anchor_postings.map(lambda x:calc_total_word_tf(x[0],x[1]))\n","inverted.term_total=totals.collectAsMap()\n","\n","inverted.write_index('.', 'anchor_inverted_index')\n","index_src = \"anchor_inverted_index.pkl\"\n","index_dst = f'gs://{bucket_name}/anchor_postings_gcp/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"markdown","id":"9282bea7","metadata":{},"source":["## Dump pkl to index folder\n","\n","\n","### method for creating a pkl and persist to bucket"]},{"cell_type":"code","execution_count":null,"id":"2077f3f3","metadata":{},"outputs":[],"source":["import pickle\n","def write_pkl_and_persist(index_src,persisted_dict):\n","#     index_src =\"doc_to_title.pkl\"\n","    with open(index_src, 'wb') as out_f:\n","        pickle.dump(persisted_dict, out_f)\n","    index_dst = f'gs://{bucket_name}/index/{index_src}'\n","    !gsutil cp $index_src $index_dst"]},{"cell_type":"markdown","id":"0ae942f1","metadata":{},"source":["## Doc_id to title\n"]},{"cell_type":"code","execution_count":null,"id":"2dc3dcea","metadata":{},"outputs":[],"source":["#create doc_id to title dict\n","doc_to_title_rdd = doc_title_pairs.flatMap(lambda x: doc_to_title(x[0],x[1]))\n","doc_id_title_rdd = doc_to_title_rdd.groupByKey().mapValues(reduce_doc_to_title)\n","doc_title_df = doc_id_title_rdd.toDF().toPandas()\n","doc_title_df['title'] = doc_title_df['_2'].apply(lambda x: x[0][0])"]},{"cell_type":"code","execution_count":null,"id":"93f35cce","metadata":{},"outputs":[],"source":["doc_id_to_title_dict = dict(zip(doc_title_df['_1'], doc_title_df['title']))\n","# import pickle\n","# index_src =\"doc_to_title.pkl\"\n","# with open(index_src, 'wb') as out_f:\n","#     pickle.dump(doc_id_to_title_dict, out_f)\n","# index_dst = f'gs://{bucket_name}/{index_src}'\n","# !gsutil cp $index_src $index_dst\n","write_pkl_and_persist(\"doc_to_title.pkl\",doc_id_to_title_dict)"]},{"cell_type":"markdown","id":"598b7a30","metadata":{},"source":["## Doc_id to Len dict\n"]},{"cell_type":"code","execution_count":null,"id":"2bc2470d","metadata":{},"outputs":[],"source":["doc_to_doc_len = doc_text_pairs.flatMap(lambda x: doc_2_len_doc(x[0],x[1]))\n","doc_lens = doc_to_doc_len.groupByKey().mapValues(reduce_doc_2_len_doc)"]},{"cell_type":"code","execution_count":null,"id":"7268f346","metadata":{},"outputs":[],"source":["count_df = doc_lens.toDF().toPandas()\n","count_df['count'] = count_df['_2'].apply(lambda x: x[0][0])"]},{"cell_type":"code","execution_count":null,"id":"704bdde6","metadata":{},"outputs":[],"source":["count_dict = dict(zip(count_df['_1'], count_df['count']))\n","# with open(path_to_drive+\"/doc_len_dict.pkl\", 'wb') as out_f:\n","#   pickle.dump(count_dict, out_f)\n","write_pkl_and_persist(\"doc_len_dict.pkl\",count_dict)"]},{"cell_type":"markdown","id":"36fd03fb","metadata":{},"source":["## Page Rank"]},{"cell_type":"code","execution_count":null,"id":"022c348b","metadata":{},"outputs":[],"source":["# we needed more workers so we updated the worker amount\n","# !gcloud dataproc clusters update cluster-7b90 \\\n","#     --region=us-central1 \\\n","#     --num-workers=4"]},{"cell_type":"code","execution_count":null,"id":"8dc00c1b","metadata":{},"outputs":[],"source":["# Page Rank\n","def generate_graph(pages):\n","#     vertices = pages.flatMap(lambda page:[(anchor[0],) for anchor in page[1]]+[(page[0],)]).distinct()\n","#     edges = pages.flatMap(lambda page:[(page[0],anchor[0]) for anchor in page[1]]).distinct()\n","    edges = pages.map(lambda page: [(page[0], link_id.id) for link_id in page[1]]).flatMap(lambda ls: ls).distinct()\n","    vertices = edges.map(lambda edge: [edge[0],edge[1]]).flatMap(lambda ls: ls).distinct().map(lambda x: (x, ))\n","    return edges, vertices"]},{"cell_type":"code","execution_count":null,"id":"aeb7a4e1","metadata":{},"outputs":[],"source":["# construct the graph \n","# pages_links_reversed = parquetFile.limit(1000).select(\"id\",\"anchor_text\").rdd\n","pages_links_reversed = parquetFile.select(\"id\",\"anchor_text\").rdd\n","edges, vertices = generate_graph(pages_links_reversed)"]},{"cell_type":"code","execution_count":null,"id":"1f954543","metadata":{},"outputs":[],"source":["page_rank_drive_folder = full_path+'/page_rank'\n","edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n"]},{"cell_type":"code","execution_count":null,"id":"78910517","metadata":{},"outputs":[],"source":["g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=10)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(pyfunc.col('pagerank').desc())\n","pr.repartition(1).write.csv(page_rank_drive_folder, compression=\"gzip\")\n","pr.show()"]},{"cell_type":"markdown","id":"cc98bad8","metadata":{},"source":["## word_idf creation\n"]},{"cell_type":"code","execution_count":null,"id":"421a2c26","metadata":{},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex, read_posting_list\n","inverted_text = InvertedIndex.read_index(base_dir= '/home/dataproc/',name='text_inverted_index')"]},{"cell_type":"code","execution_count":null,"id":"cb995ec6","metadata":{},"outputs":[],"source":["! gsutil -m cp -r \"gs://inverted_index_creation/anchor_postings_gcp\" .\n","! gsutil -m cp -r \"gs://inverted_index_creation/text_postings_gcp\" .\n","! gsutil -m cp -r \"gs://inverted_index_creation/anchor_postings_gcp\" ."]},{"cell_type":"code","execution_count":null,"id":"7ff41f25","metadata":{},"outputs":[],"source":["N = 6348910\n","# N=len(inverted_text.df)\n","# print(N)\n","import numpy as np\n","idf_calc_dict = defaultdict(float)\n","for w, w_df in inverted_text.df.items(): \n","    idf_calc_dict[w] = np.log(((N - w_df + 0.5) / (w_df + 0.5)) + 1)\n","idf_calc_dict"]},{"cell_type":"code","execution_count":null,"id":"12cdeed2","metadata":{},"outputs":[],"source":["def doc_nf(text, id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    filtered_tokens = [tok for tok in tokens if tok not in all_stopwords]\n","    doc_len = len(filtered_tokens)\n","    count = Counter(filtered_tokens)\n","    try:\n","        nf_score = np.round(1/np.sum([np.power((wc/doc_len) * idf_calc_dict.get(term, 0), 2) for \n","                                    term, wc in count.items()]),5)\n","    except KeyError as e:\n","        print(id)\n","        print(e)\n","        print(count)\n","    return [(id,float(nf_score))]\n","\n","def reduce_doc_nf(doc_nf):\n","    return doc_nf"]},{"cell_type":"code","execution_count":null,"id":"b2b2e4ee","metadata":{},"outputs":[],"source":["doc_nf_spark = doc_text_pairs.flatMap(lambda x: doc_nf(x[0],x[1]))\n","doc_nf_rdd = doc_nf_spark.groupByKey().mapValues(reduce_doc_nf)\n","\n","# doc_nf_df = doc_nf_rdd.toDF().toPandas()\n","# doc_nf_df"]},{"cell_type":"code","execution_count":null,"id":"902e2b58","metadata":{},"outputs":[],"source":["doc_nf_pd = doc_nf_rdd.toDF().toPandas()\n","doc_nf_pd['nf'] = doc_nf_pd['_2'].apply(lambda x: x[0][0])\n","doc_nf_pd"]},{"cell_type":"code","execution_count":null,"id":"76933265","metadata":{},"outputs":[],"source":["doc_id_nf_dict = dict(zip(doc_nf_pd['_1'], doc_nf_pd['nf']))\n","# with open(path_to_drive+\"/doc_len_dict.pkl\", 'wb') as out_f:\n","#   pickle.dump(count_dict, out_f)\n","write_pkl_and_persist(\"doc_id_nf.pkl\",doc_id_nf_dict)"]},{"cell_type":"markdown","id":"bccc033e","metadata":{},"source":["## PageView"]},{"cell_type":"code","execution_count":null,"id":"12f3d069","metadata":{},"outputs":[],"source":["#  Paths\n","from pathlib import Path\n","# Using user page views (as opposed to spiders and automated traffic) for the \n","# month of August 2021\n","pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n","p = Path(pv_path) \n","pv_name = p.name\n","pv_temp = f'{p.stem}-4dedup.txt'\n","pv_clean = f'{p.stem}.pkl'\n","# Download the file (2.3GB) \n","!wget -N $pv_path\n","# Filter for English pages, and keep just two fields: article ID (3) and monthly \n","# total number of page views (5). Then, remove lines with article id or page \n","# view values that are not a sequence of digits.\n","!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n","# Create a Counter (dictionary) that sums up the pages views for the same \n","# article, resulting in a mapping from article id to total page views.\n","wid2pv = Counter()\n","with open(pv_temp, 'rt') as f:\n","    for line in f:\n","        parts = line.split(' ')\n","        wid2pv.update({int(parts[0]): int(parts[1])})\n","# # write out the counter as binary file (pickle it)\n","# with open(pv_clean, 'wb') as f:\n","#     pickle.dump(wid2pv, f)\n","write_pkl_and_persist('page_views.pkl',pvclean)\n","# read in the counter\n","# with open(pv_clean, 'rb') as f:\n","#   wid2pv = pickle.loads(f.read())"]},{"cell_type":"code","execution_count":null,"id":"c02efd04","metadata":{},"outputs":[],"source":["wid2pv = Counter()\n","with open(pv_temp, 'rt') as f:\n","    for line in f:\n","        parts = line.split(' ')\n","        wid2pv.update({int(parts[0]): int(parts[1])})"]},{"cell_type":"code","execution_count":null,"id":"5bcffb6a","metadata":{},"outputs":[],"source":["# write out the counter as binary file (pickle it)\n","# with open(pv_clean, 'wb') as f:\n","#     pickle.dump(wid2pv, f)\n","write_pkl_and_persist('page_views.pkl',wid2pv)"]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}